{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Report.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"pJczFm44fHxn","colab_type":"text"},"cell_type":"markdown","source":["# Deep Reinforcement Learning: Box World Navigation\n","\n","---\n","This notebook contains the solution to the [Udacity Deep Reinforcement Learning Nano Degree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)  project one. The objective of the project is to design and implement a deep reinforcement learning agent, using the DQN algorithm, that is able to solve the custom [Unity](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)-Udacity Banana Collector environment.\n","\n","![bananaworld](https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif)\n","\n","The agent is given zero prior knowledge of the game and is tasked with learning a policy (based on the action-value function) that will allow it to maximize the  game score. The score is determined by the number and color of bananas that are collected inside the Box World: +1 for each yellow banana collected, -1 for each black banana collected. The game is considered solved upon achieving a score of +13 or higher.\n","\n","In addition to the modified Unity environment, the implementation relies on the [PyTorch](https://pytorch.org/) tensor library.\n","\n","### 1. Import Necessary Libraries and Environment"]},{"metadata":{"id":"gcevKEIUfHxo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Standard Imports\n","import numpy as np\n","import random\n","from collections import namedtuple, deque\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nevpYo9GfHxr","colab_type":"code","colab":{},"outputId":"d2048ced-2632-4ac0-8210-455096089674"},"cell_type":"code","source":["# Install and Import Unity ML-Agents\n","!pip -q install ml-agents-master/python\n","from unityagents import UnityEnvironment"],"execution_count":0,"outputs":[{"output_type":"stream","text":["You are using pip version 10.0.1, however version 18.0 is available.\n","You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"],"name":"stderr"}]},{"metadata":{"id":"-7SziQ10fHx0","colab_type":"text"},"cell_type":"markdown","source":["### 2. Load the Banana Collector Environment\n","Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."]},{"metadata":{"id":"0VOAaZBBfHxv","colab_type":"code","colab":{},"outputId":"09033eb1-e458-4531-87ff-614da23daaa1"},"cell_type":"code","source":["# Load custom Udacity Banana Brain (Unity 'Brain' <-> Environment)\n","env = UnityEnvironment(file_name=\"./Banana_Windows_x86_64/Banana.exe\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:unityagents:\n","'Academy' started successfully!\n","Unity Academy name: Academy\n","        Number of Brains: 1\n","        Number of External Brains : 1\n","        Lesson number : 0\n","        Reset Parameters :\n","\t\t\n","Unity brain name: BananaBrain\n","        Number of Visual Observations (per agent): 0\n","        Vector Observation space type: continuous\n","        Vector Observation space size (per agent): 37\n","        Number of stacked Vector Observation: 1\n","        Vector Action space type: discrete\n","        Vector Action space size (per agent): 4\n","        Vector Action descriptions: , , , \n"],"name":"stderr"}]},{"metadata":{"id":"ZaTUbGOqfHx1","colab_type":"code","colab":{}},"cell_type":"code","source":["# get the default brain\n","brain_name = env.brain_names[0]\n","brain = env.brains[brain_name]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BJfd_ZpMfHx7","colab_type":"text"},"cell_type":"markdown","source":["### 3. Get a \"Feel\" for the Banana Collector Environment\n","\n","Examine the State and Action Spaces. Run the code cell below to print some information about the environment and a sample state vector.\n","\n","Note that **in this coding environment, you will not be able to watch the agent while it is training**, and you should set `train_mode=True` to restart the environment."]},{"metadata":{"id":"lppSX-MgfHx4","colab_type":"code","colab":{},"outputId":"74987123-dbdc-4788-aef1-9ca4d07231e8"},"cell_type":"code","source":["# reset the environment\n","env_info = env.reset(train_mode=True)[brain_name]\n","\n","# number of agents in the environment\n","print('Number of agents:', len(env_info.agents))\n","\n","# number of actions\n","action_size = brain.vector_action_space_size\n","print('Number of actions:', action_size)\n","\n","# examine the state space \n","state = env_info.vector_observations[0]\n","print('States look like:', state)\n","state_size = len(state)\n","print('States have length:', state_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of agents: 1\n","Number of actions: 4\n","States look like: [1.         0.         0.         0.         0.84408134 0.\n"," 0.         1.         0.         0.0748472  0.         1.\n"," 0.         0.         0.25755    1.         0.         0.\n"," 0.         0.74177343 0.         1.         0.         0.\n"," 0.25854847 0.         0.         1.         0.         0.09355672\n"," 0.         1.         0.         0.         0.31969345 0.\n"," 0.        ]\n","States have length: 37\n"],"name":"stdout"}]},{"metadata":{"id":"flTT0ZZuNuL7","colab_type":"text"},"cell_type":"markdown","source":["Play a single episode with randomly generated actions. Learn how to use the Python API to control the agent and receive feedback from the environment."]},{"metadata":{"id":"NEp92FlXfHx7","colab_type":"code","colab":{},"outputId":"3a1a257b-867f-476a-90f9-cfe56441ca95"},"cell_type":"code","source":["env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n","state = env_info.vector_observations[0]            # get the current state\n","score = 0                                          # initialize the score\n","while True:\n","    action = np.random.randint(action_size)        # select an action\n","    env_info = env.step(action)[brain_name]        # send the action to the environment\n","    next_state = env_info.vector_observations[0]   # get the next state\n","    reward = env_info.rewards[0]                   # get the reward\n","    done = env_info.local_done[0]                  # see if episode has finished\n","    score += reward                                # update the score\n","    state = next_state                             # roll over the state to next time step\n","    if done:                                       # exit loop if episode finished\n","        break\n","    \n","print(\"Score: {}\".format(score))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Score: 1.0\n"],"name":"stdout"}]},{"metadata":{"id":"ItM9fP6rfHx-","colab_type":"text"},"cell_type":"markdown","source":["### 4. Design a Deep Reinforcement Learning (DRL) Agent\n","\n","The Deep RL Agent is an implementaiton of the Sarsamax algorithm, also known as [Deep Q-Learning (DQN)](https://deepmind.com/research/dqn/).\n","\n","The first model component of the agent is the **deep neural network** implemented with PyTorch. This model will learn an optimal mapping between the 37-dimensional input space and the 4-dimensional action space. In terms of **architecture**, the network contains:\n","- **three fully-connected (FC) hidden layers** (in a 16-32-64 hidden unit configuration) with ReLU non-linear activation functions\n","- **one fully-connected linear output layer**\n","\n","**Important Note** \n","\n","When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n","```python\n","env_info = env.reset(train_mode=True)[brain_name]\n","```"]},{"metadata":{"id":"CJwl3HErfHx_","colab_type":"code","colab":{}},"cell_type":"code","source":["# QNetwork Class\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as F\n","#import torch.optim as optim\n","\n","class QNetwork(nn.Module):\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed, fc1_units=16, fc2_units=32, fc3_units=64):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","            fc1_units (int): Number of nodes in first hidden layer\n","            fc2_units (int): Number of nodes in second hidden layer\n","        \"\"\"\n","        super(QNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.fc1 = nn.Linear(state_size, fc1_units)\n","        self.fc2 = nn.Linear(fc1_units, fc2_units)\n","        self.fc3 = nn.Linear(fc2_units, fc3_units)\n","        self.fc4 = nn.Linear(fc3_units, action_size)\n","\n","\n","    def forward(self, state):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        return self.fc4(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N3uY1MEJfHyD","colab_type":"text"},"cell_type":"markdown","source":["The second model component of the agent class, the body of the code, is the portion that interacts with the environment and iteratively learns an improved action-value function. \n","\n","Generally, the **learning process** functions as follows:\n","1. the agent receives/observes a state vector (S) from the environment\n","2. it then passes the state to the action-value network, $Q$, to decide an optimal action, $a_t^*$\n","4. after taking the action, the agent observes a resulting reward, $R_t$, and transition state, $S_{t+1}$. This sequence, $SARS'A$, is saved into a 'replay buffer'.\n","5. The agent then updates the action-value network by:\n","  - sampling observations (SARS'A-tuples) from the replay buffer\n","  - pass the sample (R, S')-tuples into a copy of the network (which mitigates instability) in order to generate an estimate of the expected discounted reward \n","  - use the estimates as target values to update the network parameters. Specifically,  minimize the mean-squared error (MSE) with gradient descent (Adam optimizer).  \n","  \n","Some elements of the algorithm deserve more concrete mathematical notation:\n","\n","- An **action-value network** is defined, $Q(S_t,A_t,w)$, that maps each state, $S_t=s_t$, to an action-values, $a_t$, according to the learned parameters, $w$. The network weights,  $w$, are initialzed randomly.\n","- Given the action-value function $Q(S_t,A_t,w)$, **actions are selected** at each step according to:\n","$$a_t^* = argmax_{a_t \\in A(s_t)} Q(S_t=s_t, A_t=a_t)$$\n","- The **objective function** used to train the model (find the optimal $w$) is:\n","$$ argmax_w \\Theta = (Q(s_t, a_t, w^-) - \\hat{Q}(s_t, a_t^*, w))^2 $$\n","where the 'true' target values (left term) are estimated according to:\n","$$Q(s_t, a_t, w^-) = R_t + \\gamma \\hat{Q}(s_{t+1}, a_{t+1}^*, w^-)$$\n","Note that $w^-$ is a **temporally-stable copy of the primary network weights**.\n","\n","  \n","Certain **meta-parameters** must be specified and tuned to achieve optimal performance:\n","- Buffer Size: 100,000 samples\n","- Batch Size: 64\n","- $\\gamma$: Discount factor for future rewards\n","- Learning Rate: 5E-5\n","- Update Target Model: Every 4 steps\n","- $\\tau$: controls how aggressively the target model is updated/blended with the local model\n","- $\\epsilon_{start}$: Starting epsilon value for determining how often to select the random action vs the greedy action\n","- $\\epsilon_{end}$:  Terminal epsilon value for determining how often to select the random action vs the greedy action\n","- $\\epsilon_{decay}$:  Parameter determining how quickly the epsilon value transitions during training\n"]},{"metadata":{"id":"Wl0g5tk8fHyE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Agent Class\n","#import numpy as np\n","#import random\n","#from collections import namedtuple, deque\n","\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as F\n","#import torch.optim as optim\n","\n","#from model import QNetwork\n","\n","BUFFER_SIZE = int(1e5)  # replay buffer size\n","BATCH_SIZE = 64         # minibatch size\n","GAMMA = 0.99            # discount factor\n","TAU = 1e-3              # for soft update of target parameters\n","#LR = 5e-4               # learning rate \n","LR = 5e-5               # learning rate \n","UPDATE_EVERY = 4        # how often to update the network\n","\n","#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","\n","class Agent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.seed = random.seed(seed)\n","\n","        # Q-Network\n","        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n","        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n","\n","        # Replay memory\n","        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n","        # Initialize time step (for updating every UPDATE_EVERY steps)\n","        self.t_step = 0\n","    \n","    def step(self, state, action, reward, next_state, done):\n","        # Save experience in replay memory\n","        self.memory.add(state, action, reward, next_state, done)\n","        \n","        # Learn every UPDATE_EVERY time steps.\n","        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n","        if self.t_step == 0:\n","            # If enough samples are available in memory, get random subset and learn\n","            if len(self.memory) > BATCH_SIZE:\n","                experiences = self.memory.sample()\n","                self.learn(experiences, GAMMA)\n","\n","    def act(self, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy.\n","        \n","        Params\n","        ======\n","            state (array_like): current state\n","            eps (float): epsilon, for epsilon-greedy action selection\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        self.qnetwork_local.eval()\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)\n","        self.qnetwork_local.train()\n","\n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","\n","    def learn(self, experiences, gamma):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","\n","        Params\n","        ======\n","            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n","            gamma (float): discount factor\n","        \"\"\"\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        # Get max predicted Q values (for next states) from target model\n","        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n","        # Compute Q targets for current states \n","        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n","\n","        # Get expected Q values from local model\n","        Q_expected = self.qnetwork_local(states).gather(1, actions)\n","\n","        # Compute loss\n","        loss = F.mse_loss(Q_expected, Q_targets)\n","        # Minimize the loss\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n","\n","    def soft_update(self, local_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        ?_target = ?*?_local + (1 - ?)*?_target\n","\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n","\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, action_size, buffer_size, batch_size, seed):\n","        \"\"\"Initialize a ReplayBuffer object.\n","\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=buffer_size)  \n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.seed = random.seed(seed)\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","    \n","    def sample(self):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        experiences = random.sample(self.memory, k=self.batch_size)\n","\n","        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n","        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n","        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n","        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n","        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n","  \n","        return (states, actions, rewards, next_states, dones)\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OEcEBNXgfHyG","colab_type":"code","colab":{}},"cell_type":"code","source":["# instantiate agent\n","#from dqn_agent import Agent\n","agent = Agent(state_size=37, action_size=4, seed=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O6jytVtafHyJ","colab_type":"code","colab":{},"outputId":"9a78eb27-3eb5-4552-e9e1-bfbbfdc6a1fe"},"cell_type":"code","source":["def dqn(n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n","    \"\"\"Deep Q-Learning.\n","    \n","    Params\n","    ======\n","        n_episodes (int): maximum number of training episodes\n","        max_t (int): maximum number of timesteps per episode\n","        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n","        eps_end (float): minimum value of epsilon\n","        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n","    \"\"\"\n","    scores = []                        # list containing scores from each episode\n","    scores_window = deque(maxlen=100)  # last 100 scores\n","    eps = eps_start                    # initialize epsilon\n","    for i_episode in range(1, n_episodes+1):\n","        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n","        state = env_info.vector_observations[0]            # get the current state\n","        score = 0\n","        for t in range(max_t):\n","            action = int(agent.act(state, eps))\n","            \n","            env_info = env.step(action)[brain_name]        # send the action to the environment\n","            next_state = env_info.vector_observations[0]   # get the next state\n","            reward = env_info.rewards[0]                   # get the reward\n","            done = env_info.local_done[0]                  # see if episode has finished\n","            \n","            agent.step(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","            #print(t,'reward: ',reward, 'score: ',score,'done: ',done)\n","            \n","            if done:                                       # exit loop if episode finished\n","                break\n","\n","        scores_window.append(score)       # save most recent score\n","        scores.append(score)              # save most recent score\n","        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n","        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n","        if i_episode % 100 == 0:\n","            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","        if np.mean(scores_window)>=13.0:\n","            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","            break\n","    return scores\n","\n","scores = dqn(max_t=750)\n","\n","# plot the scores\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(len(scores)), scores)\n","plt.ylabel('Score')\n","plt.xlabel('Episode #')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 100\tAverage Score: 0.36\n","Episode 200\tAverage Score: 2.78\n","Episode 300\tAverage Score: 4.96\n","Episode 400\tAverage Score: 7.19\n","Episode 500\tAverage Score: 10.01\n","Episode 600\tAverage Score: 12.93\n","Episode 613\tAverage Score: 13.00\n","Environment solved in 513 episodes!\tAverage Score: 13.00\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"iXVeiSnHfHyL","colab_type":"text"},"cell_type":"markdown","source":["When finished, you can close the environment."]},{"metadata":{"id":"-nlvD2-7fHyM","colab_type":"code","colab":{}},"cell_type":"code","source":["env.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"psRirTNzz-uL","colab_type":"text"},"cell_type":"markdown","source":["### Future Directions\n","The training algorithm could benefit from some specific enhancements. \n","- [Double Q-Learning](https://arxiv.org/abs/1509.06461): DQN tends to overestimate the action-values. This can be partially remedied by separating the action-value estimation function from the action-selection function.\n","- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952): Some observations are more important than others. i.e. observations with very large positive or negative action-values. By sampling more heavily from these outsized-payoff observations, the algorithm is able to learn more efficiently.\n","- [Dueling DQN](https://arxiv.org/abs/1511.06581): Decompose the action-values, $Q(s,a)$ estimates into a state-value prediction, $V(s)$, and an action-advantage-values, $A(s,a)$: $$Q(s,a) = V(s) + A(s,a)$$.\n","- [Rainbow](https://arxiv.org/abs/1710.02298): Combining all of these enhancements into one product in addition to a number of other extensions (multi-strep bootstrap targets, distributional dqn, noisy dqn)"]}]}